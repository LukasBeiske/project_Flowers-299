{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7iucdvlQH3m",
    "outputId": "8f8204b8-277c-4430-e5ea-69ea4e88166e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n",
      "Python version = 3.7.10\n"
     ]
    }
   ],
   "source": [
    "#Check if colab is running\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    %tensorflow_version 2.x\n",
    "    \n",
    "#import TF  \n",
    "import tensorflow as tf\n",
    "from platform import python_version\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "print(\"Python version =\",python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xt_9jfl6Q-Jd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1338)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.constrained_layout.use\" : True,\n",
    "    \"font.size\" : 14,\n",
    "    \"figure.figsize\" : (7, 5)\n",
    "})\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsPf2r1PSpf3",
    "outputId": "ec847f33-7222-4f1b-9d1f-61a43136c7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM9O1gOEGQpE"
   },
   "outputs": [],
   "source": [
    "path = \"drive/MyDrive/ML_Projekt/ML_Flowers\"\n",
    "flowers = [\"ButterflyBush\", \"GrapeHyacinth\", \"BalloonFlower\", \"BleedingHeart\", \"FrangipaniFlower\", \"Daisy\", \"Black-eyedSusan\", \"BlanketFlower\", \"Waterlilies\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rz9txiW-hYVV"
   },
   "source": [
    "# Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnACn7lTF98Q"
   },
   "outputs": [],
   "source": [
    "shapes = []\n",
    "images = []\n",
    "for flower in flowers:\n",
    "    for entry in os.scandir(path + f\"/Original/{flower}/\"):\n",
    "        im = np.array(Image.open(entry.path))\n",
    "        images.append(im)\n",
    "        shapes.append(im.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHb52WTQHcEb"
   },
   "outputs": [],
   "source": [
    "shapes = np.array(shapes)\n",
    "ratio = shapes[:,0] / shapes[:,1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(22, 5))\n",
    "axs[0].scatter(shapes[:,1], shapes[:,0], marker=\"x\")\n",
    "axs[0].set(\n",
    "    xlabel = \"width\",\n",
    "    ylabel = \"height\",\n",
    "    aspect = 1\n",
    ")\n",
    "for ax, value, label in zip(axs[1:], \n",
    "                            [shapes[:,0], shapes[:,1], ratio],\n",
    "                            [\"height\", \"width\", \"height / width\"]):\n",
    "    ax.hist(value, bins=50, histtype=\"stepfilled\")\n",
    "    ax.set(\n",
    "        xlabel = label,\n",
    "        ylabel = \"#images\",\n",
    "        aspect = \"auto\"\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TM-IfHFZtow"
   },
   "outputs": [],
   "source": [
    "# same as above using seaborn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_shapes = pd.DataFrame({\n",
    "    \"height\" : shapes[:,0],\n",
    "    \"width\" : shapes[:,1]\n",
    "})\n",
    "\n",
    "sns.jointplot(df_shapes.height, df_shapes.width, kind=\"scatter\", height=7, ratio=3)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(ratio, bins=50, histtype=\"stepfilled\")\n",
    "ax.set(\n",
    "    xlabel = \"height / width\",\n",
    "    ylabel = \"#images\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz8VBa_ruYO7"
   },
   "source": [
    "### Define maximum Dimensions of images to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx-GtEq1oXOh"
   },
   "outputs": [],
   "source": [
    "lower = 60/100\n",
    "upper = 1/lower\n",
    "mask = np.logical_and(ratio > lower, ratio < upper)\n",
    "print(f\"Von {len(shapes)} Bildern haben {len(shapes[mask])} ein Seitenverhältnis zwischen {lower} und {upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4m236Cm1oRDj"
   },
   "outputs": [],
   "source": [
    "print(\"Die Indizes einiger sehr breiter Bilder:\", np.where(ratio < lower))\n",
    "print(\"Die Indizes einiger sehr hoher Bilder:\", np.where(ratio > upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkIWOyqxcKoi"
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[144]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtdB36xFjTAz"
   },
   "outputs": [],
   "source": [
    "# How to get size of image from a PIL Image object\n",
    "testpath = f\"drive/MyDrive/ML_Projekt/ML_Flowers/Original/{flowers[0]}\"\n",
    "\n",
    "for i, entry in enumerate(os.scandir(testpath)):\n",
    "    if i == 129:\n",
    "        im = Image.open(entry.path)\n",
    "        print(im.getbbox()) #(left, upper, right, lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBET_251N-WN"
   },
   "source": [
    "# Resize pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WVbMscVVeVc"
   },
   "outputs": [],
   "source": [
    "### Resizen von Bildern auf 200x200\n",
    "img_rows = 200\n",
    "img_cols = 200\n",
    "shape_ord = (img_rows, img_cols, 3)\n",
    "\n",
    "lower = 60/100\n",
    "upper = 1/lower\n",
    "\n",
    "if not os.path.isdir(path + \"/Resized/\"):\n",
    "    os.mkdir(path + \"/Resized/\")\n",
    "\n",
    "for flower in flowers:\n",
    "    if not os.path.isdir(path + \"/Resized/\" + flower):\n",
    "        folder = path + \"/Resized/\" + flower\n",
    "        os.mkdir(folder)\n",
    "        for entry in os.scandir(path + f\"/Original/{flower}/\"):\n",
    "            im = Image.open(entry.path)\n",
    "            # only use images with: lower < ratio < upper\n",
    "            ratio = im.getbbox()[3] / im.getbbox()[2] # height / width\n",
    "            if ratio > lower and ratio < upper:\n",
    "                im_r = im.resize((img_rows, img_cols), Image.ANTIALIAS)\n",
    "                im_r.save(path + f\"/Resized/{flower}/\" + entry.path.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7BL-8Z-f6Nb"
   },
   "source": [
    "# Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TzjSkiwjgwKn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "newpath = path + \"/Resized/\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i, flower in enumerate(flowers):\n",
    "    for entry in os.scandir(newpath + flower):\n",
    "        X.append(np.array(Image.open(entry.path)))\n",
    "        y.append(i)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "y = to_categorical(y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjIa27eSyKt5",
    "outputId": "7ad4059a-22b9-43bd-b2cd-c34fedfc6860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3281, 200, 200, 3) [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Für die einzelnen Klassen sind so viele Instanzen im Datensatz:\n",
      " ['ButterflyBush', 'GrapeHyacinth', 'BalloonFlower', 'BleedingHeart', 'FrangipaniFlower', 'Daisy', 'Black-eyedSusan', 'BlanketFlower', 'Waterlilies'] \n",
      " [409. 368. 435. 411. 318. 282. 407. 326. 325.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y[0])\n",
    "print(\"Für die einzelnen Klassen sind so viele Instanzen im Datensatz:\\n\", \n",
    "      flowers, \"\\n\", np.sum(y, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6qCiINlm3Bw"
   },
   "source": [
    "### Split Test/ Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO0M513iqHP6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_test, y_Train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y) # 15% test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxHd2C4bm6Vy"
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuLRPtpiowmN",
    "outputId": "bc66bf39-2557-491e-cc93-070d032a1e9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2788, 120000)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train = X_Train.reshape(X_Train.shape[0], img_rows * img_cols * 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols * 3)\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wJG-4aEowmU",
    "outputId": "8a3bf0e7-c6c3-4de4-bd83-17b9f1ca20b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2788, 120000)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_Train = X_Train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "X_Train = scaler.fit_transform(X_Train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwKg1seJmxQi",
    "outputId": "55f0fbd2-dec6-44f9-ac1e-39f2cb692186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2788, 200, 200, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train = X_Train.reshape(X_Train.shape[0],img_rows,img_cols, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0],img_rows,img_cols, 3)\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Jhj4ghHw_L"
   },
   "source": [
    "# Plotting and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pALK_jdaHttQ"
   },
   "outputs": [],
   "source": [
    "# Plotting functions adapted from exercise 5\n",
    "\n",
    "def plot_history(network_history):\n",
    "    fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    ax1.set(\n",
    "      xlabel = \"Epochs\",\n",
    "      ylabel = \"Loss\" \n",
    "    )\n",
    "    ax1.plot(network_history.history['loss'], label=\"Training\")\n",
    "    ax1.plot(network_history.history['val_loss'], label=\"Validation\")\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.set(\n",
    "      xlabel = \"Epochs\",\n",
    "      ylabel = \"Accuracy\" \n",
    "    )\n",
    "    ax2.plot(network_history.history['accuracy'], label=\"Training\")\n",
    "    ax2.plot(network_history.history['val_accuracy'], label=\"Validation\")\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_errors(errors_index, img_errors, pred_errors, obs_errors, img_rows, img_cols):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels \"\"\"\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, axs = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "      error = errors_index[i]\n",
    "      ax.imshow(img_errors[error].reshape(img_rows, img_cols, 3),\n",
    "                interpolation='nearest')\n",
    "      ax.set_title(f\"Predicted label :{pred_errors[error]}\\nTrue label :{obs_errors[error]}\")\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWLMODAkLN3O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate(X_test, Y_test, label):\n",
    "    label_name = flowers[label]\n",
    "\n",
    "    ### Evaluate loss and metrics\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test Loss:', loss)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    # Predict the values from the test dataset\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Convert one hot vectors to classes \n",
    "    Y_cls = np.argmax(Y_pred, axis = 1)\n",
    "    Y_true = np.argmax(Y_test, axis = 1) \n",
    "    print('Classification Report:\\n', classification_report(Y_true, Y_cls))\n",
    "    \n",
    "    ### Plot 0 probability\n",
    "    Y_pred_prob = Y_pred[:, label]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(Y_pred_prob[Y_true == label], alpha=0.5, color='red', \n",
    "             bins=10, log=True, label=f\"True label is {label_name}\")\n",
    "    plt.hist(Y_pred_prob[Y_true != label], alpha=0.5, color='blue',\n",
    "             bins=10, log=True, label=f\"True label is not {label_name}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(f'Probability of Label {label_name}')\n",
    "    plt.ylabel('Number of entries')\n",
    "    plt.show()\n",
    "    \n",
    "    ### compute and plot the confusion matrix\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_cls) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = flowers)\n",
    "\n",
    "    ### Plot largest errors\n",
    "    errors = (Y_cls - Y_true != 0)\n",
    "    Y_cls_errors = Y_cls[errors]\n",
    "    Y_pred_errors = Y_pred[errors]\n",
    "    Y_true_errors = Y_true[errors]\n",
    "    X_test_errors = X_test[errors]\n",
    "\n",
    "    # Probabilities of the wrong predicted labels\n",
    "    Y_pred_errors_prob = np.max(Y_pred_errors, axis = 1)\n",
    "    # Predicted probabilities of the true values in the error set\n",
    "    true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "    \n",
    "    # Difference between the probability of the predicted label and the true label\n",
    "    delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "    sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "    # Top 6 errors \n",
    "    most_important_errors = sorted_dela_errors[-6:]\n",
    "    display_errors(most_important_errors, X_test_errors, Y_cls_errors, Y_true_errors,\n",
    "                   X_test.shape[1], X_test.shape[2])\n",
    "    print(\"Biggest error probabilities are: \", Y_pred_errors_prob[most_important_errors])\n",
    "    \n",
    "    ### Plot predictions\n",
    "    predicted = Y_cls[:15]\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=5, figsize=(12, 6))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(X_test[i].reshape(img_rows, img_cols, 3),\n",
    "                  interpolation='nearest')\n",
    "        ax.text(0, 0, flowers[predicted[i]], \n",
    "                color='black', bbox=dict(facecolor='white', alpha=1))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9XZo2cXnA9t"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZmOjhW9nulk",
    "outputId": "987301cb-06fe-4a45-82b7-96ad7effdab6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2296, 200, 200, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_Train, y_Train, test_size=0.1764, random_state=42, stratify=y_Train) # 15% validation set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4vHXuDUzNaR",
    "outputId": "51ade166-5137-44b4-8f35-2393bc0df7d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Für die einzelnen Klassen sind so viele Instanzen im Validierungsdatensatz:\n",
      " ['ButterflyBush', 'GrapeHyacinth', 'BalloonFlower', 'BleedingHeart', 'FrangipaniFlower', 'Daisy', 'Black-eyedSusan', 'BlanketFlower', 'Waterlilies'] \n",
      " [61. 55. 65. 62. 48. 42. 61. 49. 49.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Für die einzelnen Klassen sind so viele Instanzen im Validierungsdatensatz:\\n\", \n",
    "      flowers, \"\\n\", np.sum(y_val, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQx-gCiMlUPf",
    "outputId": "069d542d-bb22-4c6c-deff-71f680a3c2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 197, 197, 64)      3136      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 98, 98, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 95, 95, 32)        32800     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 16)        8208      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               991360    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 1,053,177\n",
      "Trainable params: 1,053,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "nb_epoch = 40\n",
    "batch_size = 128\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# number of nodes for dense layers\n",
    "dense_nodes = 128\n",
    "# dropout rate\n",
    "rate_do = 0.3 \n",
    "\n",
    "# convolution kernel size\n",
    "nb_conv = 4\n",
    "#size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "\n",
    "\n",
    "# Manually trying some structures leads to something like this working quite well \n",
    "# (2 dense layers does improve the result)\n",
    "layers_base = [\n",
    "    Conv2D(nb_filters, kernel_size=nb_conv, padding='valid', activation='relu', input_shape=shape_ord),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Conv2D(nb_filters / 2, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Conv2D(nb_filters / 4, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Flatten(),\n",
    "    Dropout(rate_do),\n",
    "    Dense(dense_nodes, activation='relu'),\n",
    "    Dropout(rate_do),\n",
    "    Dense(dense_nodes, activation='relu'),\n",
    "    Dropout(rate_do),\n",
    "    Dense(len(flowers), activation='softmax')\n",
    "]\n",
    "\n",
    "\n",
    "model = Sequential(layers_base)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "5hQdsCVoom5L",
    "outputId": "559eebc2-8954-4c7c-e4e2-05f55a8a07bf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cab5ab40f618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistoryEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mval_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistoryEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class HistoryEpoch(Callback):\n",
    "    def __init__(self, data):\n",
    "        self.data = data        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.data\n",
    "        l, a = self.model.evaluate(x, y, verbose=0)\n",
    "        self.loss.append(l)\n",
    "        self.acc.append(a)\n",
    "\n",
    "\n",
    "train_hist = HistoryEpoch((X_train, y_train))\n",
    "val_hist = HistoryEpoch((X_val, y_val))\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size = batch_size, \n",
    "                 epochs = nb_epoch, verbose=1, \n",
    "                 validation_data = (X_val, y_val),\n",
    "                 callbacks = [val_hist, train_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXiUqTraOheZ"
   },
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZkEHL39LWQU"
   },
   "source": [
    "# Hyperparametertuning mit Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRkEOGq-PnzA"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def build_model_from_params(params):\n",
    "    layers = [\n",
    "        Conv2D(params[\"num_filters\"], kernel_size=nb_conv, padding='valid', activation='relu', input_shape=shape_ord),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Conv2D(params[\"num_filters\"] / 2, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Conv2D(params[\"num_filters\"] / 4, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Flatten(),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(params[\"dense_nodes\"], activation='relu'),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(params[\"dense_nodes\"], activation='relu'),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(len(flowers), activation='softmax')\n",
    "    ]\n",
    "  \n",
    "    model = Sequential(layers)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "\n",
    "param_space = {\n",
    "    'batch_size': [128, 256],\n",
    "    'num_filters': [32, 64],\n",
    "    'dropout': [0.3, 0.4, 0.5],\n",
    "    'dense_nodes': [64, 128]\n",
    "}\n",
    "\n",
    "value_combis = itertools.product(*[v for v in param_space.values()])\n",
    "\n",
    "param_combis = [{key:value for key, value in zip(param_space.keys(), combi)} for combi in value_combis]\n",
    "\n",
    "print(f\"Es werden {len(param_combis)} Parameterkombination getestet:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jk_ne4t1d5L"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for idx, params in enumerate(param_combis):\n",
    "    print(f\"Starte Run {idx+1}/{len(param_combis)} mit den Parameteren: {params}\")\n",
    "\n",
    "    string_config = \"\"\n",
    "    for key, value in params.items():\n",
    "      string_config += \"_\" + key + \"=\" + str(value)\n",
    "\n",
    "    # save best model according to validation accuracy\n",
    "    filepath = f\"paramsearch{string_config}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor=\"val_accuracy\", verbose=0, save_best_only=True, mode=\"max\"\n",
    "    )\n",
    "\n",
    "    model = build_model_from_params(params)\n",
    "    \n",
    "    # train the model\n",
    "    fit_results = model.fit(\n",
    "        x = X_train, y = y_train, \n",
    "        batch_size = params[\"batch_size\"], epochs = nb_epoch, \n",
    "        validation_data = (X_val, y_val), \n",
    "        callbacks = [checkpoint],\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "    # extract the best validation scores\n",
    "    best_val_epoch    = np.argmax(fit_results.history['val_accuracy'])\n",
    "    best_val_acc      = np.max(fit_results.history['val_accuracy'])\n",
    "    best_val_acc_loss = fit_results.history['val_loss'][best_val_epoch]\n",
    "\n",
    "    # get correct training accuracy\n",
    "    best_model = load_model(filepath)\n",
    "    best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(X_train, y_train)\n",
    "\n",
    "    # store results\n",
    "    search_results.append({\n",
    "        **params,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_loss': best_val_acc_loss,\n",
    "        'best_train_acc': best_val_acc_train_acc,\n",
    "        'best_train_loss': best_val_acc_train_loss,\n",
    "        'best_val_epoch': best_val_epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAXs0OUaZXLX"
   },
   "outputs": [],
   "source": [
    "resultsDF = pd.DataFrame(search_results)\n",
    "resultsDF.sort_values('best_val_acc', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
