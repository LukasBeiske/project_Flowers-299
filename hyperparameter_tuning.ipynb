{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3bb89cd",
   "metadata": {},
   "source": [
    "### This notebook can only be run in colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ee630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if colab is running\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    %tensorflow_version 2.x\n",
    "    \n",
    "#import TF  \n",
    "import tensorflow as tf\n",
    "from platform import python_version\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "print(\"Python version =\",python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6f5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "np.random.seed(1338)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.constrained_layout.use\" : True,\n",
    "    \"font.size\" : 14,\n",
    "    \"figure.figsize\" : (7, 5)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd27e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e79f9",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9482c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2441, 200, 200, 3), (2441,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers = [\n",
    "    \"ButterflyBush\",\n",
    "    \"GrapeHyacinth\",\n",
    "    \"BalloonFlower\",\n",
    "    \"BleedingHeart\",\n",
    "    \"FrangipaniFlower\",\n",
    "    \"Daisy\",\n",
    "    \"Black-eyedSusan\",\n",
    "    \"BlanketFlower\",\n",
    "    \"Waterlilies\"\n",
    "]\n",
    "img_rows = 200\n",
    "img_cols = 200\n",
    "shape_ord = (img_rows, img_cols, 3)\n",
    "\n",
    "\n",
    "with h5py.File(\"data/images_resized_train.h5\", \"r\") as file:\n",
    "    X = np.array(file[\"/images\"]).astype(\"float32\")\n",
    "    y = np.array(file[\"/labels\"]).astype(\"uint8\")\n",
    "    \n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "y = to_categorical(y, num_classes)\n",
    "\n",
    "print(y[0])\n",
    "print(\"FÃ¼r die einzelnen Klassen sind so viele Instanzen im Datensatz:\\n\", \n",
    "      flowers, \"\\n\", np.sum(y, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921fdcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2010, 200, 200, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split 15% of total data as validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1764, random_state=42, stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc8cfd",
   "metadata": {},
   "source": [
    "# Plotting and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c29f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions adapted from exercise 5\n",
    "import itertools\n",
    "\n",
    "def plot_history(network_history):\n",
    "    fig, [ax1, ax2] = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    ax1.set(\n",
    "      xlabel = \"Epochs\",\n",
    "      ylabel = \"Loss\" \n",
    "    )\n",
    "    ax1.plot(network_history.history['loss'], label=\"Training\")\n",
    "    ax1.plot(network_history.history['val_loss'], label=\"Validation\")\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.set(\n",
    "      xlabel = \"Epochs\",\n",
    "      ylabel = \"Accuracy\" \n",
    "    )\n",
    "    ax2.plot(network_history.history['accuracy'], label=\"Training\")\n",
    "    ax2.plot(network_history.history['val_accuracy'], label=\"Validation\")\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_errors(errors_index, img_errors, pred_errors, obs_errors, img_rows, img_cols):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels \"\"\"\n",
    "    fig, axs = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "      error = errors_index[i]\n",
    "      ax.imshow(img_errors[error].reshape(img_rows, img_cols, 3),\n",
    "                interpolation='nearest')\n",
    "      ax.set_title(f\"Predicted label :{pred_errors[error]}\\nTrue label :{obs_errors[error]}\")\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5861111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate(X_test, Y_test, label):\n",
    "    label_name = flowers[label]\n",
    "\n",
    "    ### Evaluate loss and metrics\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test Loss:', loss)\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    # Predict the values from the test dataset\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Convert one hot vectors to classes \n",
    "    Y_cls = np.argmax(Y_pred, axis = 1)\n",
    "    Y_true = np.argmax(Y_test, axis = 1) \n",
    "    print('Classification Report:\\n', classification_report(Y_true, Y_cls))\n",
    "    \n",
    "    ### Plot 0 probability\n",
    "    Y_pred_prob = Y_pred[:, label]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(Y_pred_prob[Y_true == label], alpha=0.5, color='red', \n",
    "             bins=10, log=True, label=f\"True label is {label_name}\")\n",
    "    plt.hist(Y_pred_prob[Y_true != label], alpha=0.5, color='blue',\n",
    "             bins=10, log=True, label=f\"True label is not {label_name}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(f'Probability of Label {label_name}')\n",
    "    plt.ylabel('Number of entries')\n",
    "    plt.show()\n",
    "    \n",
    "    ### compute and plot the confusion matrix\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_cls) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = flowers)\n",
    "\n",
    "    ### Plot largest errors\n",
    "    errors = (Y_cls - Y_true != 0)\n",
    "    Y_cls_errors = Y_cls[errors]\n",
    "    Y_pred_errors = Y_pred[errors]\n",
    "    Y_true_errors = Y_true[errors]\n",
    "    X_test_errors = X_test[errors]\n",
    "\n",
    "    # Probabilities of the wrong predicted labels\n",
    "    Y_pred_errors_prob = np.max(Y_pred_errors, axis = 1)\n",
    "    # Predicted probabilities of the true values in the error set\n",
    "    true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "    \n",
    "    # Difference between the probability of the predicted label and the true label\n",
    "    delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "    sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "    # Top 6 errors \n",
    "    most_important_errors = sorted_dela_errors[-6:]\n",
    "    display_errors(most_important_errors, X_test_errors, Y_cls_errors, Y_true_errors,\n",
    "                   X_test.shape[1], X_test.shape[2])\n",
    "    print(\"Biggest error probabilities are: \", Y_pred_errors_prob[most_important_errors])\n",
    "    \n",
    "    ### Plot predictions\n",
    "    predicted = Y_cls[:15]\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=5, figsize=(12, 6))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(X_test[i].reshape(img_rows, img_cols, 3),\n",
    "                  interpolation='nearest')\n",
    "        ax.text(0, 0, flowers[predicted[i]], \n",
    "                color='black', bbox=dict(facecolor='white', alpha=1))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ac2c2",
   "metadata": {},
   "source": [
    "# Train first network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "nb_epoch = 40\n",
    "batch_size = 128\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64\n",
    "# number of nodes for dense layers\n",
    "dense_nodes = 128\n",
    "# dropout rate\n",
    "rate_do = 0.3 \n",
    "\n",
    "# convolution kernel size\n",
    "nb_conv = 4\n",
    "#size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "\n",
    "\n",
    "# Manually trying some structures leads to something like this working quite well \n",
    "# (2 dense layers does improve the result)\n",
    "layers_base = [\n",
    "    Conv2D(nb_filters, kernel_size=nb_conv, padding='valid', activation='relu', input_shape=shape_ord),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Conv2D(nb_filters / 2, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Conv2D(nb_filters / 4, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "    MaxPooling2D(pool_size=nb_pool),\n",
    "    Flatten(),\n",
    "    Dropout(rate_do),\n",
    "    Dense(dense_nodes, activation='relu'),\n",
    "    Dropout(rate_do),\n",
    "    Dense(dense_nodes, activation='relu'),\n",
    "    Dropout(rate_do),\n",
    "    Dense(len(flowers), activation='softmax')\n",
    "]\n",
    "\n",
    "\n",
    "model = Sequential(layers_base)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])          \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class HistoryEpoch(Callback):\n",
    "    def __init__(self, data):\n",
    "        self.data = data        \n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.data\n",
    "        l, a = self.model.evaluate(x, y, verbose=0)\n",
    "        self.loss.append(l)\n",
    "        self.acc.append(a)\n",
    "\n",
    "\n",
    "train_hist = HistoryEpoch((X_train, y_train))\n",
    "val_hist = HistoryEpoch((X_val, y_val))\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size = batch_size, \n",
    "                 epochs = nb_epoch, verbose=1, \n",
    "                 validation_data = (X_val, y_val),\n",
    "                 callbacks = [val_hist, train_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277929d",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c24820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_params(params):\n",
    "    layers = [\n",
    "        Conv2D(params[\"num_filters\"], kernel_size=nb_conv, padding='valid', activation='relu', input_shape=shape_ord),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Conv2D(params[\"num_filters\"] / 2, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Conv2D(params[\"num_filters\"] / 4, kernel_size=nb_conv, padding='valid', activation='relu'),\n",
    "        MaxPooling2D(pool_size=nb_pool),\n",
    "        Flatten(),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(params[\"dense_nodes\"], activation='relu'),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(params[\"dense_nodes\"], activation='relu'),\n",
    "        Dropout(params[\"dropout\"]),\n",
    "        Dense(len(flowers), activation='softmax')\n",
    "    ]\n",
    "  \n",
    "    model = Sequential(layers)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "\n",
    "param_space = {\n",
    "    'batch_size': [128, 256],\n",
    "    'num_filters': [32, 64],\n",
    "    'dropout': [0.3, 0.4, 0.5],\n",
    "    'dense_nodes': [64, 128]\n",
    "}\n",
    "\n",
    "value_combis = itertools.product(*[v for v in param_space.values()])\n",
    "\n",
    "param_combis = [{key:value for key, value in zip(param_space.keys(), combi)} for combi in value_combis]\n",
    "\n",
    "print(f\"Es werden {len(param_combis)} Parameterkombination getestet:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for idx, params in enumerate(param_combis):\n",
    "    print(f\"Starte Run {idx+1}/{len(param_combis)} mit den Parameteren: {params}\")\n",
    "\n",
    "    string_config = \"\"\n",
    "    for key, value in params.items():\n",
    "      string_config += \"_\" + key + \"=\" + str(value)\n",
    "\n",
    "    # save best model according to validation accuracy\n",
    "    filepath = f\"paramsearch{string_config}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath, monitor=\"val_accuracy\", verbose=0, save_best_only=True, mode=\"max\"\n",
    "    )\n",
    "\n",
    "    model = build_model_from_params(params)\n",
    "    \n",
    "    # train the model\n",
    "    fit_results = model.fit(\n",
    "        x = X_train, y = y_train, \n",
    "        batch_size = params[\"batch_size\"], epochs = nb_epoch, \n",
    "        validation_data = (X_val, y_val), \n",
    "        callbacks = [checkpoint],\n",
    "        verbose = 0\n",
    "    )\n",
    "\n",
    "    # extract the best validation scores\n",
    "    best_val_epoch    = np.argmax(fit_results.history['val_accuracy'])\n",
    "    best_val_acc      = np.max(fit_results.history['val_accuracy'])\n",
    "    best_val_acc_loss = fit_results.history['val_loss'][best_val_epoch]\n",
    "\n",
    "    # get correct training accuracy\n",
    "    best_model = load_model(filepath)\n",
    "    best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(X_train, y_train)\n",
    "\n",
    "    # store results\n",
    "    search_results.append({\n",
    "        **params,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_loss': best_val_acc_loss,\n",
    "        'best_train_acc': best_val_acc_train_acc,\n",
    "        'best_train_loss': best_val_acc_train_loss,\n",
    "        'best_val_epoch': best_val_epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e65a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = pd.DataFrame(search_results)\n",
    "resultsDF.sort_values('best_val_acc', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
